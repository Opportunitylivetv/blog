<html>
  <head>
    <title>ETech '07 Summary - Part 2 - MegaData</title>
    <meta name=created value="2007-04-05T10:35:04.780590">
  </head>
  <body>
  <p>
Here's the thing, we need a new kind of data store,
a new kind of SQL, something that does for storing and
querying large amounts of data what SQL did for normalized data.
</p>
<p><b>Update:</b> <a href="http://bitworking.org/news/159/Megadata-Follow-up">Follow-up here</a>.</p>
  <p>
Sure you can store a lot of data in a relational database, but 
when I say large, I mean really large; a billion or more records.
I know we need this because I keep seeing people build it.
</p>
<h3 id="examples">Examples in the wild</h3>
<p>Here are some of the examples I've seen recently, 
  most of which was brought together by my attendance
  at ETech:
</p>
<dl>
<dt>
<a href="http://jeffjonas.typepad.com/">Jeff Jonas</a>
</dt>
<dd>
    The work Jeff does is pretty amazing and you should read
    <a href="http://jeffjonas.typepad.com/TTI-Vanguard-NextGen-Technologies.pdf">this paper</a> 
    to get an idea of the kinds of analysis 
    he's doing. One thing you don't see in that paper, but I did
    see on one of his slides at ETech, was the limits you need to put on 
    yourself when storing a billion rows in a database, and they
    included: no joins, no transactions, no stored procedures, and no triggers.
    </dd>
<dt><a href="http://joshua.schachter.org/">Joshua Schacter</a></dt>
<dd>
        Joshua has similar suggestions from his experience
        building del.icio.us: no joins, no transactions,
        <a href="http://joshua.schachter.org/2007/01/autoincrement.html">no autoincrement</a>.
    </dd>
<dt><a href="http://labs.google.com/papers/bigtable.html">BigTable</a></dt>
<dd>
    I have talked previously about BigTable, Google's column-based store with no transactions.
    Yahoo! has their own technology similar to BigTable.
    </dd>
<dt>eBay</dt>
<dd>
<a href="http://www.addsimplicity.com/downloads/eBaySDForum2006-11-29.pdf">Slides from Randy Shoup and Dan Pritchett</a>
    show how eBay runs with referential integrity, joins, and sorting moved
    out of the database and into the application.
    </dd>
</dl>
<p>I'm detecting a pattern here.</p>
<h3 id="themes">Common Themes</h3>
<p>Some common themes are emerging. If you want to scale to the
petabyte level, or the billion requests a day, you
need to be:
</p>
<dl>
<dt>Distributed</dt>
<dd>The data has to be distributed across multiple machines.</dd>
<dt>Joinless</dt>
<dd>No joins, and no referential integrity, at least at the data store level.</dd>
<dt>De-Normalized</dt>
<dd>No one said this explicily, but I presume there is a lot of de-normalization going on if you are 
        avoiding joins.
    </dd>
<dt>Transcationless</dt>
<dd>No transactions</dd>
</dl>
<p>Those constraints represent something fundamentally different
from a relational database.</p>
<h3 id="usage">Who would buy that?</h3>
<p>If we build something like that who would use it? There are only so many Googles and eBays in the
world, right?.</p>
<blockquote><p><a href="http://www.quotationspage.com/quote/33253.html">Thomas Watson:</a> I think there is a world market for maybe five computers.</p></blockquote>
<p>We are barely scratching the surface of data today.
More and more people are going to be going 
through this, which is what I've been pointing out
lately, for example, when 
<a href="http://bitworking.org/news/142/Unevenly-distributed">I asked how many motors
 and things with IP addresses you had in your house</a>.
We're already averaging 12 IPs per household. When we counted motors 
we got around 80 to 100. There are going to be more and more
devices with IP addresses in your house, and many of 
those devices will actually be <b>generating</b> data, continuously.
</p>
<p>Here is just one simple example.
How often is your electrical usage measured? Once a month?
What if it was measured every second?
Combine that the Jeff Jonas anaylysis/anonymization work
and <a href="http://en.wikipedia.org/wiki/Jeff_Hawkins">Jeff Hawkin's Hierarchical Temporal Memory</a> and do you have something
that might be able to predict energy consumption? Maybe pick out 
some abberations on the grid? Preemptively offer you 
discounts for peak periods? 
</p>
<p>I don't know what's possible, or even useful, when it 
comes to monitoring and distributing electricity. The only
thing I <i>do</i> know is that there's a lot of potential for
disruption here.
</p>
<p>
But let's not get sidetracked by the example, 
this isn't a call for green tech, or an intelligent 
grid. This is a call to think about the data collection
and processing farms that are going to have to be built,
and the kind of data store required, to use
all that data.
</p>
<p>There are obvious privacy, security, and 
social issues of all this data collection
and this has to be thought through before
building these systems. For just one example, think about the 
publicity when Al Gore's utility bill was
made public. That was just a simple set of 
monthly measurements of his gas and electricity
usage. 
</p>
<h3 id="developers">Think of the developers!</h3>
<p>So what about the poor developers that need
to develop on top this rather strange platform, how
will they fare? From <a href="http://labs.google.com/papers/bigtable.html">Bigtable: A Distributed Storage System for Structured Data</a>
</p>
<blockquote><p>
     Given the unusual interface to Bigtable, an interest-
ing question is how difﬁcult it has been for our users to
adapt to using it. New users are sometimes uncertain of
how to best use the Bigtable interface, particularly if they
are accustomed to using relational databases that support
general-purpose transactions. Nevertheless, the fact that
many Google products successfully use Bigtable demon-
strates that our design works well in practice.
</p></blockquote>
<p>The only difference between today
and two years ago when Adam Bosworth gave
his talk <a href="http://www.itconversations.com/shows/detail571.html">Database Requirements in the Age of Scalable Services</a>
is that there's a lot more public knowledge about
what the likes of Google and eBay are doing.
</p>
  <div class="commentContent" id="X1">
    <blockquote cite="http://bitworking.org/news/158/ETech-07-Summary-Part-2-MegaData" type="cite"><p>...we need a new kind of data store, a new kind of SQL, something that does for storing andquerying large amounts of data what SQL did for normalized data</p></blockquote>
<p>How about RDF and SPARQL?  Doesn't "Distributed, Joinless, De-Normalized, and Transcationless" pretty much describe RDF + SPARQL + distributed data sources from around the web?  There are implementations capable of handling millions of triples (see <a href="http://www.ldodds.com/blog/archives/000216.html">http://www.ldodds.com/blog/archives/000216.html</a>, <a href="http://morenews.blogspot.com/2004/02/kowari-101.html">http://morenews.blogspot.com/2004/02/kowari-101.html</a>, or <a href="http://www.lassila.org/blog/archive/2006/03/bigger_datasets.html">http://www.lassila.org/blog/archive/2006/03/bigger_datasets.html</a>, for example).   </p>
<p><a href="http://www.dehora.net/journal/2005/06/hitting_reload_is_the_framework_job.html">Way back in the day</a>, Bill de hÓra said:</p>
<blockquote cite="http://www.dehora.net/journal/2005/06/hitting_reload_is_the_framework_job.html" type="cite"><p>I'm telling myself I'm using RDF+XML because I want to be able to pull data in from anywhere. That's true, but to be brutally honest I can't be bothered designing and maintaining yet another relational schema for yet another webapp - doing so is starting to make as much sense as designing my own filesystem or TP monitor. Life's too short, too short to be working on technology that can only possibly make sense when you're in dressed in combats and vans listening to Pearljam pretending it's still the nineties... there's a real wish to conduct oneself at a higher level of abstraction before complete dementia sets in. What's the point in designing tables for a webapp when an RDF-backed store will manage the data for you and RDF queries will come back as tabular data anyway?</p></blockquote>
    <p class="commentByLine">Posted by
       <a href="mailto:dlc@sevenroot.org">Darren Chamberlain</a> on <a href="#X1" title="2007-04-05T11:48:27.756156">2007-04-05</a>
    </p>
</div><div class="commentContent" id="X2">
    <p>"Think of the developers!"</p>
<p>To heck with 'em. I just want someone to figure out how to manage my own personal terabyte's worth: <a href="http://www.dehora.net/journal/2006/04/bdd_backup_driven_development.html">backup driven development.</a>
</p><p>Pilgrim had a much better rant a while back about backing up. It's hard to be sure you have everything backed up. Or worse you end up with duplicated files on each computer (gee thanks, itunes). </p>
<p>I really am no more than 6 months away from needing a personal datacenter. I've been putting it off for about 2 years. And I don't even take pictures of flowers; it's ridiculous.  Sometimes I think I was better off with photonegatives in shoeboxes.</p>
    <p class="commentByLine">Posted by
       <a href="mailto:bill@dehora.net">Bill de hÓra</a> on <a href="#X2" title="2007-04-05T14:03:51.901492">2007-04-05</a>
    </p>
</div><div class="commentContent" id="X3">
    <p>"How about RDF and Sparql?"</p>
<p>Darren, you're right. RDF can be partitioned N ways to Sunday; it's massively scalable. The naive rdbms design for RDF happens to be a SmallTable - 3 columns and N rows, divisible at any point in the rowset. BigTable as I understand it, might be more usable tho'. </p>
<p>Since I wrote that rant you quoted, I've seen two problems, that have given me pause:</p>
<ul>
<li>domain models. developers can't work easily with arbitrary xml, so arbitrary graphs are right out. It's much easier if everything is a row in a table or an object. I think this is part of the reason that rails and django are so productive; they're highly optimised for domain models. Raw RDF doesn't really do domains like that; you have to expend effort distilling triples into 'things'; witness how much angst WS marshalling causes; very few developers will give up on objects</li>
<li>decentralised  search. I think whoever figures out peered search will change everything. Right now it's all about downloading the web into a datacenter and distributing the index across a fast WAN. The Sparql people seem to be thinking along the same lines - everything will be behind some uber-endpoint a la Google.  Doug Cutting (nutch, lucene) once said the reason decentralised search doesn't work is latency. That's the key; figure out a way to get people not to care about latency (eg by providing higher precision), and you'll disrupt search.  </li>
</ul>
    <p class="commentByLine">Posted by
       <a href="mailto:bill@dehora.net">Bill de hÓra</a> on <a href="#X3" title="2007-04-05T14:34:35.277294">2007-04-05</a>
    </p>
</div><div class="commentContent" id="X4">
    Have you read Rohit Khare's dissertation?  He uses the same electrical grid example.  http://www.ics.uci.edu/~rohit/DecentralizingREST.pdf
    <p class="commentByLine">Posted by
       <a href="http://www.markbaker.ca">Mark Baker</a> on <a href="#X4" title="2007-04-05T15:16:41.289702">2007-04-05</a>
    </p>
</div><div class="commentContent" id="X5">
    Millions of triples in an RDF-backed stores is still several orders of magnitude away from the requirements. Since RDF normalises everything to triples, you need many more triples than you do rows in a DB, so the stress test case is thousand times smaller than a giga-store. Not that you can't put an RDF front end on it, but RDF isn't itself a store technology, and a community that considers 10 million data points a stress test isn't likely to produce a giga-store any time soon.

The problem with triples is you then have to do joins to create objects out of them (such as an address), and joins cost, especially when distributed.

Even with everything in memory, the RDF stores linked to above seem slow (one would expect a simple query on a in-memory 10 million triple store not to take more than 100ms on modern hardware - use 32 bit hashs and GPU mask operations, and write the hashed form to a binary dump at disk access speed). The quoted store takes '15 minutes to load about 150MB' - it takes 5s to read 150MB from disk on my laptop into memory; any representation that adds a couple of orders of magnitude to IO isn't going to scale past physical memory limits. rdfs:subClassOf either turns a simple query into a recursive one, or adds many extra triples to the store. 

RDF isn't the answer to how to implement a giga-store; it just begs the question, and adds recursive joins to the requirements. 

(I've no argument against the niceness of a flexible schema, so I'd like something like RDF to work, but RDF doesn't seem to be heading in that direction very quickly)


Pete
    <p class="commentByLine">Posted by
       <a href="http://tincancamera.com">Pete Kirkham</a> on <a href="#X5" title="2007-04-05T17:28:00.893221">2007-04-05</a>
    </p>
</div><div class="commentContent" id="X6">
    Mark, unfortunately (at least for me) dereferencing the URL http://www.ics.uci.edu/~rohit/DecentralizingREST.pdf results in a 403 response.

But the following, formatted for duplex printing, is (all 30 Mb of it!):

http://www.ics.uci.edu/~rohit/Khare-Thesis-Duplex.pdf

The following related papers are also accessible (and more modest in size):

http://www.ics.uci.edu/~rohit/ARRESTED-ICSE.pdf
http://www.isr.uci.edu/tech_reports/UCI-ISR-03-8.pdf
    <p class="commentByLine">Posted by
       <a href="http://blogs.sun.com/sandoz/">Paul Sandoz</a> on <a href="#X6" title="2007-04-06T07:09:23.037927">2007-04-06</a>
    </p>
</div><div class="commentContent" id="X7">
    "Joinless, De-Normalized, Transcationless"
<br /><br />
From the architectures described by Google and eBay, this is clearly the case at the data store level, but in fact joins and transactions still exist conceptually at what is now considered the application layer. Transactions are accomplished via work flow and state machines and data joined by stitching data together in memory.
<br /><br />
Joins and transactions could be lifted out of the vertically-scalable data store and into some new horizontally-scalable "join/transaction" layer in a general way, but it would involve a whole new set of concepts for the application programmer to consider.
<br /><br />
The proposed join/transaction layer would need to stitch information together using the some of the techniques described as "Level 5 Routing" by Phil Windley a few years ago:
<br /><br />
<a href="http://www.windley.com/archives/2002/12/level_5_routing.shtml">Level 5 Routing for Web Services</a>
<br /><br />
Interestingly, the kind of routers listed there map pretty closely to the principal motivators behind AOP...  I'll bet the metadata used to describe the joins and transactions at this layer would look a lot like aspects, though perhaps not with that name applied.
    <p class="commentByLine">Posted by
       <a href="http://www.xefer.com">Winter</a> on <a href="#X7" title="2007-04-06T13:26:53.107827">2007-04-06</a>
    </p>
</div><div class="commentContent" id="X8">
    Oops.  That link to Rohit's dissertation should be working now.
    <p class="commentByLine">Posted by
       <a href="http://www.markbaker.ca">Mark Baker</a> on <a href="#X8" title="2007-04-06T23:46:06.647112">2007-04-06</a>
    </p>
</div><div class="commentContent" id="X9">
    I've been working on a framework to solve problems like this using Amazon Web Services (EC2/S3) and SQLite. It's theoretically infinitely scalable and I just described its basic architecture in this post: <a href="http://blog.nanobeepers.com/2007/04/07/infinitely-scalable-framework-with-aws/">http://blog.nanobeepers.com/2007/04/07/infinitely-scalable-framework-with-aws/</a>
<br /><br />
Would love to hear feedback on it :)
    <p class="commentByLine">Posted by
       <a href="http://blog.nanobeepers.com/">Matt Jaynes</a> on <a href="#X9" title="2007-04-07T07:25:09.923805">2007-04-07</a>
    </p>
</div><div class="commentContent" id="X10">
    Joe,

We are planning to release a beta that will let people deploy bigtable style databases (scale-out indices) in about a month.  The project site is:
<p></p>
<a href="http://sourceforge.net/projects/bigdata/">http://sourceforge.net/projects/bigdata/</a>
<p></p>
The license will be the same as BerkeleyDB or MySQL -- GPL with a FOSS (Free and Open Source) exclusion.
<p></p>
There is nothing there right now -- I'm planning to import the code concurrent with the first beta release.

-bryan
    <p class="commentByLine">Posted by
       <a href="mailto:thompsonbry@users.sourceforge.net">Bryan Thompson</a> on <a href="#X10" title="2007-04-07T10:32:50.247564">2007-04-07</a>
    </p>
</div><div class="commentContent" id="X11">
    <p>" Millions of triples in an RDF-backed stores is still several orders of magnitude away from the requirements. Since RDF normalises everything to triples, you need many more triples than you do rows in a DB, so the stress test case is thousand times smaller than a giga-store"</p>
<p>Pete, my experience has been one order of magnitude (10^2) blowup when storing in RDF instead of domain models a la RDBMS, and it's the rule of thumb I tell others. The thing is, RDF data is probably easier to partition than RDBMS backed data, so the question is whether 10^2 is a real deterrent relative to storage costs and latency of queries/joins. And technically I suspect RDF data can be partitioned dynamically, like the way we move compute jobs around today. </p>
<p>I doubt it's as easy to split something like the canonical user or items tables in a Web2.0 app.</p>
    <p class="commentByLine">Posted by
       <a href="http://dehora.net/journal">Bill de hÓra</a> on <a href="#X11" title="2007-04-08T12:27:05.847493">2007-04-08</a>
    </p>
</div><div class="commentContent" id="X12">
    <p>Real rdf that describes real world things is going to end up organized as nodes that look like hubs - the hubs will have lots of connections hanging off them, and their structure will be repetitive, if most of the data is of the same kind.  There are likely to be plenty of bnode hubs, as well.</p>
<p>Each hub looks a lot like a row in a table.  Yes, you could partition the triples differently, but for a given application, you probably won't.</p>
<p>In this way of looking at things, an rdf graph is essentially a highly normalized relational database.  Now we know that to get real-world performance, you usually have to denormalize a database.  The implemented schema may not resemble the conceptual one very much at all.</p>
<p>Where am I going with this?  The huge databases being discussed here are presumably highly denormalized to get performance.  I don't see any reason why you couldn't have a denormalized implementation for an rdf dataset.  You design these implementations with your specific computing needs in mind, and they are no longer general-purpose systems.</p>
<p>IOW, the use of huge, denormalized, table implementations can be orthogonal to the question of whether to "use rdf", if that's what you want.  But you probably can't have them and also have general-purpose data stores at the same time.  Just like it always has been.</p>
    <p class="commentByLine">Posted by
       <a href="mailto:pub1@tompassin.net">Thomas B Passin</a> on <a href="#X12" title="2007-04-08T14:27:31.984792">2007-04-08</a>
    </p>
</div><div class="commentContent" id="X13">
    Thomas. WRT:

"I don't see any reason why you couldn't have a denormalized implementation for an rdf dataset.".

ARC is an RDF database that allows just that.  You can partition your RDF space such that your application benefits.  

http://bnode.org/blog/2006/02/20/arc-rdf-store-for-php-ensparql-your-lamp

I"ve done something similar with my relational model by breaking out 'is-a' relations.
    <p class="commentByLine">Posted by
       <a href="http://metacognition.info">Chimezie</a> on <a href="#X13" title="2007-04-09T08:01:47.641230">2007-04-09</a>
    </p>
</div><div class="commentContent" id="X14">
    Dynamic partitioning of rdf data works fine.  We will be releasing a module for scale-out triple/quad stores over the basic bigdata framework. -bryan
    <p class="commentByLine">Posted by
       <a href="mailto:bryan@systap.com">bryan thompson</a> on <a href="#X14" title="2007-04-09T13:13:05.776244">2007-04-09</a>
    </p>
</div><div class="commentContent" id="X16">
    Bosworth was thinking along these lines a while back:

<a href="http://acmqueue.com/modules.php?name=Content&amp;pa=showpage&amp;pid=337&amp;page=5">acm queue</a>
    <p class="commentByLine">Posted by
       <a href="http://www.dancres.org/blitzblog">Dan Creswell</a> on <a href="#X16" title="2007-04-10T09:26:38.314109">2007-04-10</a>
    </p>
</div><div class="commentContent" id="X17">
    Dan,<br />
<p>Yes, the ITConversations link I posted predates that ACM article 
by six months.
</p>
    <p class="commentByLine">Posted by
       <a href="http://bitworking.org">Joe</a> on <a href="#X17" title="2007-04-10T10:29:10.493964">2007-04-10</a>
    </p>
</div>
  </body>